\section{Evaluation}

\subsection{Evaluation Metrics for Classification}

\subsubsection{Precision}
Precision is the percentage of positive identifications that were classified correctly.\newline
\begin{equation}
\small
    Precision\ =\ \frac{True\ Positives}{True\ Positives\ +\ False\ Positives}
\end{equation}
\normalsize
\subsubsection{Recall}
Recall is the percentage of all actual positives that were classified correctly.
\small
\begin{equation}
    Recall\ =\ \frac{True\ Positives}{True\ Positives\ +\ False\ Negatives}
\end{equation}
\normalsize
\subsubsection{F1-Score}
F1-Score is the harmonic mean of precision and recall.
\small
\begin{equation}
    F1-Score\ =\ 2 \times\ \frac{Precision\ \times\ Recall}{Precision\ +\ Recall}
\end{equation}
\normalsize
\subsubsection{Accuracy}
Accuracy is the percentage of the tweets that were classified correctly.
\small
\begin{equation}
    Accuracy\ =\ \frac{Number\ of\ Correct\ Predictions}{Total\ Number\ of\ Predictions}
\end{equation}
\normalsize

\subsection{Classification Results and Analysis}

\begin{table}[h!]
\setlength{\tabcolsep}{3pt}
\caption{Precision of Classifiers and Feature Representations.}
\label{Table:precision}
\resizebox{\columnwidth}{!}{
\begin{tabular}{cccccccc}
\specialrule{1.25pt}{1pt}{1pt}
 & \textbf{BOW} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Unigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Bigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Trigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Unigram,\\ No Stop Words)\end{tabular}} & \textbf{Word2Vec} & \textbf{Doc2Vec} \\ \specialrule{1.25pt}{1pt}{1pt}
\textbf{RF} & 0.72 & 0.74 & 0.7 & 0.69 & 0.73 & 0.71 & 0.59  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{DT} & 0.57 & 0.58 & 0.58 & 0.59 & 0.63 & 0.54 & 0.56  \\ \hline
\textbf{MLP} & 0.71 & 0.71 & 0.7 & 0.7 & 0.7 & 0.71 & 0.59  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{SVM} & 0.56 & 0.74 & 0.73 & 0.71 & 0.71 & 0.71 & 0.62  \\ \hline
\textbf{LR} & 0.72 & 0.72 & 0.7 & 0.68 & 0.7 & 0.71 & 0.61  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{KNN} & 0.53 & 0.62 & 0.62 & 0.62 & 0.61 & 0.67 & 0.54  \\ \hline
\textbf{GP} & 0.72 & 0.72 & 0.71 & 0.69 & 0.7 & 0.72 & 0.61  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{AB} & 0.63 & 0.62 & 0.63 & 0.62 & 0.64 & 0.63 & 0.57  \\ \hline
\textbf{GNB} & 0.6 & 0.6 & 0.63 & 0.63 & 0.59 & 0.64 & 0.59  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{MNB} & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}0.71} & 0.67 & 0.68 & 0.66 & 0.67 & 0.32 & 0.35    \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{BNB} & 0.73 & 0.73 & 0.71 & 0.71 & 0.72 & 0.65 & 0.58  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{QDA} & 0.72 & 0.76 & 0.73 & 0.67 & \textcolor{red}{0.79} & 0.59 & 0.57  \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{LDA} & 0.63 & 0.64 & 0.65 & 0.63 & 0.62 & 0.69 & 0.66  \\ \hline
\end{tabular}}
\end{table}

In terms of precision, QDA with unigram TF-IDF (no stop words) performed best (Table ~\ref{Table:precision}). RF and SVM, both with unigram TF-IDF  were the two next best performing classifiers. Our results do not fully confirm or contradict the results of \cite{Rane2018}. \cite{Rane2018} found that RF achieved the highest precision score, followed by AB and SVM. Their precision scores are higher for all the classifiers they implemented. A likely reason for this is that they trained their classifiers on a larger dataset (14640 tweets), while our classifiers were trained on a smaller set (3115 tweets). Supervised machine learning methods tend to perform the best on large datasets.

\begin{table}[h!]
\setlength{\tabcolsep}{3pt}
\caption{Recall of Classifiers and Feature Representations.}
\label{Table:recall}
\resizebox{\columnwidth}{!}{
\begin{tabular}{cccccccc}
\specialrule{1.25pt}{1pt}{1pt}
 & \textbf{BOW} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Unigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Bigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Trigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Unigram,\\ No Stop Words)\end{tabular}} & \textbf{Word2Vec} & \textbf{Doc2Vec} \\ \specialrule{1.25pt}{1pt}{1pt}
\textbf{RF} & 0.7 & 0.71 & 0.7 & 0.69 & 0.72 & 0.69 & 0.64  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{DT} & 0.61 & 0.6 & 0.61 & 0.62 & 0.65 & 0.56 & 0.59  \\ \hline
\textbf{MLP} & 0.72 & 0.72 & 0.71 & 0.71 & 0.71 & 0.72 & 0.62  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{SVM} & 0.59 & \textcolor{red}{0.74} & \textcolor{red}{0.74} & 0.72 & 0.72 & 0.72 & 0.64  \\ \hline
\textbf{LR} & 0.72 & 0.72 & 0.71 & 0.7 & 0.71 & 0.72 & 0.64  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{KNN} & 0.61 & 0.66 & 0.66 & 0.65 & 0.65 & 0.68 & 0.61  \\ \hline
\textbf{GP} & 0.73 & 0.73 & 0.72 & 0.71 & 0.71 & 0.73 & 0.64  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{AB} & 0.65 & 0.64 & 0.65 & 0.65 & 0.66 & 0.64 & 0.61  \\ \hline
\textbf{GNB} & 0.48 & 0.48 & 0.47 & 0.45 & 0.48 & 0.56 & 0.49  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{MNB} & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}0.66} & 0.68 & 0.69 & 0.67 & 0.68 & 0.57 & 0.59  \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{BNB} & 0.71 & 0.71 & 0.66 & 0.62 & 0.71 & 0.56 & 0.54  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{QDA} & 0.26 & 0.23 & 0.24 & 0.56 & 0.26 & 0.61 & 0.68  \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{LDA} & 0.61 & 0.61 & 0.61 & 0.59 & 0.59 & 0.69 & 0.67  \\ \hline
\end{tabular}}
\end{table}

SVM performed best with regard to recall (Table ~\ref{Table:recall}), with unigram TF-IDF and bigram TF-IDF performing equally well. This again partly agrees with \cite{Rane2018}, who found that SVM was their second best performing classifier in terms of recall, with RF again coming out on top. Notice that QDA which performed the best in precision performs pretty poorly in terms of recall. This is a common issue and is why we have also evaluated the classifiers with f1-score, which conveys the balance between precision and recall.

\begin{table}[h!]
\setlength{\tabcolsep}{3pt}
\caption{F1-Score of Classifiers and Feature Representations.}
\label{Table:f1score}
\resizebox{\columnwidth}{!}{
\begin{tabular}{cccccccc}
\specialrule{1.25pt}{1pt}{1pt}
 & \textbf{BOW} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Unigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Bigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Trigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Unigram,\\ No Stop Words)\end{tabular}} & \textbf{Word2Vec} & \textbf{Doc2Vec} \\ \specialrule{1.25pt}{1pt}{1pt}
\textbf{RF} & 0.65 & 0.65 & 0.64 & 0.64 & 0.69 & 0.63 & 0.59  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{DT} & 0.57 & 0.59 & 0.58 & 0.6 & 0.63 & 0.54 & 0.56  \\ \hline
\textbf{MLP} & 0.71 & 0.7 & 0.69 & 0.69 & 0.69 & 0.71 & 0.57  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{SVM} & 0.45 & \textcolor{red}{0.73} & \textcolor{red}{0.73} & 0.71 & 0.71 & 0.71 & 0.63  \\ \hline
\textbf{LR} & 0.71 & 0.7 & 0.69 & 0.68 & 0.69 & 0.71 & 0.6  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{KNN} & 0.49 & 0.62 & 0.62 & 0.62 & 0.61 & 0.67 & 0.55  \\ \hline
\textbf{GP} & 0.71 & 0.71 & 0.72 & 0.7 & 0.7 & 0.72 & 0.61  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{AB} & 0.63 & 0.62 & 0.63 & 0.62 & 0.63 & 0.63 & 0.59  \\ \hline
\textbf{GNB} & 0.5 & 0.51 & 0.49 & 0.47 & 0.5 & 0.58 & 0.51  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{MNB} & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}0.67} & 0.63 & 0.66 & 0.65 & 0.64 & 0.41 & 0.44 \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{BNB} & 0.72 & 0.72 & 0.67 & 0.64 & 0.71 & 0.58 & 0.55  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{QDA} & 0.23 & 0.19 & 0.19 & 0.48 & 0.23 & 0.55 & 0.61  \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{LDA} & 0.62 & 0.62 & 0.62 & 0.6 & 0.6 & 0.69 & 0.66  \\ \hline
\end{tabular}}
\end{table}

SVM achieved the highest f1-score (Table ~\ref{Table:f1score}), with unigram and bigram TF-IDF again performing equally well. This indicates that SVM has a good balance of precision and recall and confirms the results of both \cite{Raithi2018} and \cite{Rane2018} who both found that SVM performed well in classifying tweets. It contradicts the results of \cite{Berm2010} who found that MNB performed better than SVM on tweets. In our experiments, SVM outperformed MNB in all metrics. This difference in results could be due to the increase in character count, from 140 to 280, introduced by Twitter in 2017. \cite{Berm2010} found that SVM outperformed MNB on long form text so perhaps the character count increase means that tweets are now more similar to long form text.

\begin{table}[h!]
\setlength{\tabcolsep}{3pt}
\caption{Accuracy of Classifiers and Feature Representations.}
\label{Table:accuracy}
\resizebox{\columnwidth}{!}{
\begin{tabular}{cccccccc}
\specialrule{1.25pt}{1pt}{1pt}
 & \textbf{BOW} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Unigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Bigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Trigram)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}TF-IDF\\ (Unigram,\\ No Stop Words)\end{tabular}} & \textbf{Word2Vec} & \textbf{Doc2Vec} \\ \specialrule{1.25pt}{1pt}{1pt}
\textbf{RF} & 0.703 & 0.706 & 0.696 & 0.691 & 0.721 & 0.677 & 0.635  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{DT} & 0.614 & 0.596 & 0.614 & 0.621 & 0.652 & 0.563 & 0.594  \\ \hline
\textbf{MLP} & 0.719 & 0.719 & 0.711 & 0.711 & 0.709 & 0.718 & 0.621  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{SVM} & 0.594 & \textcolor{red}{0.744} & 0.737 & 0.718 & 0.724 & 0.716 & 0.637  \\ \hline
\textbf{LR} & 0.724 & 0.721 & 0.709 & 0.696 & 0.713 & 0.722 & 0.639  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{KNN} & 0.608 & 0.655 & 0.657 & 0.652 & 0.649 & 0.681 & 0.609  \\ \hline
\textbf{GP} & 0.726 & 0.726 & 0.724 & 0.706 & 0.708 & 0.729 & 0.644  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{AB} & 0.654 & 0.644 & 0.650 & 0.650 & 0.655 & 0.637 & 0.609  \\ \hline
\textbf{GNB} & 0.478 & 0.483 & 0.473 & 0.448 & 0.479 & 0.565 & 0.486  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{MNB} & \multicolumn{1}{c}{\cellcolor[HTML]{EFEFEF}0.660} & 0.683 & 0.691 & 0.675 & 0.678 & 0.569 & 0.588  \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{BNB} & 0.713 & 0.713 & 0.662 & 0.621 & 0.709 & 0.565 & 0.537  \\ \hline
\rowcolor[HTML]{EFEFEF} 
\textbf{QDA} & 0.263 & 0.235 & 0.238 & 0.558 & 0.261 & 0.612 & 0.678  \\ \hline
\rowcolor[HTML]{FFFFFF} 
\textbf{LDA} & 0.609 & 0.614 & 0.609 & 0.588 & 0.591 & 0.693 & 0.665  \\ \hline
\end{tabular}}
\end{table}

SVM also achieved the highest accuracy (Table ~\ref{Table:accuracy}). This means it had the highest percentage of correct predictions.

Overall the best performing classifier was SVM. SVM has a good balance of precision and recall, achieving the highest f1-score, and also the highest accuracy score. A possible reason that SVM achieved the top performance is that SVM tends to be effective in high dimensional feature spaces and Twitter data produces a large feature set.

The best performing approach was a combination of SVM and unigram TF-IDF. TF-IDF seems to perform particularly well in combination with SVM, significantly improving its performance. However, TF-IDF does not conclusively improve tweet classification performance across all the classifiers. This may be due to the short length of the tweets. Short text is likely to have noisy TF-IDF values whereas the binary occurrence information obtained through plain BOW is more stable.

Increasing the number of N-grams did not improve the classification precision, accuracy, recall or f1-score. This agrees with the findings of \cite{Berm2010}. Neither Doc2Vec or Word2Vec achieved any better performance than BOW or TF-IDF. Word2Vec achieved better results in classifying the tweets than Doc2Vec. This could be because Word2Vec used Google's pretrained model whereas the Doc2Vec model was trained on our small dataset of 3115 tweets. Finally, stop word removal did not improve classification performance.

\subsection{Evaluation Metrics for Recommender Systems}
\subsubsection{Leave-One-Out}
Leave-One-Out involves removing one of a users n hotel bookings. Their remaining hotel bookings are used to generate the hotel recommendations. The removed booking is compared to the ranked list of hotel recommendations to see where it ranked, the higher the better. 

\subsubsection{Mean Percentile Rank}
Mean Percentile Rank (MPR) is a recall-based measure. It measures the user satisfaction of items in an ordered list.
\begin{equation}
    MPR = \frac{ \sum_{u,i} r_{ui} \times rank_{ui} } {\sum_{u,i} r_{ui}}
\end{equation}

$rank_{ui}$ is the percentile-ranking of hotel i within the ordered list of all hotels ranked for user u. $r_{ui}$ is a binary variable indicating whether user u booked hotel i.

\subsubsection{User Data}
The Ryanair booking data that was used to evaluate CoRE \cite{core2019} was used in our evaluation. The Ryanair dataset consists of 29,704 hotel bookings. This project focused on hotels in Dublin so the dataset had to be filtered so that it only contained users with multiple hotel bookings, one of which was in Dublin. Multiple hotel bookings are required for the 'leave-one-out' approach so that a booking can be removed while still leaving at least one booking to use for generating the recommendations. There were 27 users who had multiple hotel bookings with at least one hotel booking in a Dublin hotel. The Dublin hotel bookings were for 21 unique hotels. 

\subsubsection{Baselines}
In order to assess the performance of SentiCoRE (CoRE with the added sentiment information), it was evaluated against multiple baselines. These included: 
\begin{itemize}
    \item Random: A randomly ranked list of hotels.
    \item Expedia Baseline: The approach currently used by Ryanair in their rooms booking site. Hotels from the target city are sorted based on their transactional data taken from Expedia for the last 30 days.
    \item CoRE: The CoRE recommender system without the sentiment score added in.
    \item SentiCoRE: The CoRE recommender system re-ranked with the sentiment scores.
\end{itemize}

\subsection{Recommender Results and Analysis}

The results show that the addition of the sentiment score increases the MPR of CoRE, with and without feature weighting (Table ~\ref{Table:senticore}). This means the hotels are being ranked lower by SentiCoRE than CoRE. You can see that incorporating the sentiment scores into the CoRE recommender definitely has an impact on the hotel rankings. A positive sentiment score will move a hotel up the list while a negative ranking will move a hotel down the list. Taking only the MPR into account you would say that SentiCoRE is not performing as well as CoRE. However, SentiCoRE is having the desired effect and is adjusting the hotel rankings based on the sentiment scores. Both CoRE and SentiCoRE performed significantly better than the Random and Expedia baselines.

\begin{table}[h!]
\caption{Recommender Systems Performance.}
\label{Table:senticore}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ll}
\specialrule{1.2pt}{1pt}{1pt}
\textbf{Recommender System} & \textbf{MPR (\%)} \\ \specialrule{1.2pt}{1pt}{1pt}
\rowcolor[HTML]{EFEFEF}
Random & 42.72 \\ \hline
Expedia Baseline & 40.87 \\ \hline
\rowcolor[HTML]{EFEFEF}
CoRE (with feature weighting) & 2.51 \\ \hline
CoRE (without feature weighting) & 3.31 \\ \hline
\rowcolor[HTML]{EFEFEF}
SentiCoRE (with feature weighting) & 14.68 \\ \hline
SentiCoRE (without feature weighting) & 9.92 \\ \hline
\end{tabular}}
\end{table}

The small number of users that the evaluation was performed on needs to be taken into consideration. Our results would hold more weight if the system were reevaluated on a larger dataset. This is an area for future work.